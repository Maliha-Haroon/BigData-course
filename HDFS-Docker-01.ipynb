{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf8ea139-6ad9-4378-b4ea-b5499a22fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db85c157-c78f-4f45-96d6-c9e0ad8d71a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session (works on Windows)\n",
    "spark = SparkSession.builder.appName(\"HDFS Reader - Windows\") \\\n",
    "    .master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4493020-7d97-451e-a4c3-c30c712ee14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import requests\n",
    "from urllib.parse import urlparse, urlunparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "612eba43-7152-472b-809d-5a609e92436f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDFSClient:\n",
    "    def __init__(self, host='localhost', namenode_port=9870, datanode_port=9865):\n",
    "        self.host = host\n",
    "        self.namenode_port = namenode_port\n",
    "        self.datanode_port = datanode_port\n",
    "        self.base_url = f'http://{host}:{namenode_port}/webhdfs/v1'\n",
    "    \n",
    "    def _fix_datanode_url(self, url):\n",
    "        \"\"\"Replace container hostname with localhost\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        new_netloc = f'{self.host}:{self.datanode_port}'\n",
    "        \n",
    "        fixed = urlunparse((\n",
    "            parsed.scheme, new_netloc, parsed.path,\n",
    "            parsed.params, parsed.query, parsed.fragment\n",
    "        ))\n",
    "        \n",
    "        return fixed\n",
    "    \n",
    "    def read_file(self, hdfs_path):\n",
    "        \"\"\"Read file from HDFS with redirect handling\"\"\"\n",
    "        url = f'{self.base_url}{hdfs_path}?op=OPEN'\n",
    "        response = requests.get(url, allow_redirects=False)\n",
    "        \n",
    "        if response.status_code == 307:\n",
    "            redirect_url = response.headers['Location']\n",
    "            fixed_url = self._fix_datanode_url(redirect_url)\n",
    "            data_response = requests.get(fixed_url)\n",
    "            return data_response.content\n",
    "        elif response.status_code == 200: return response.content\n",
    "        else:\n",
    "            raise Exception(f\"Error: {response.status_code} - {response.text}\")\n",
    "    \n",
    "    def list_directory(self, path='/'):\n",
    "        \"\"\"List contents of HDFS directory\"\"\"\n",
    "        url = f'{self.base_url}{path}?op=LISTSTATUS'\n",
    "        response = requests.get(url)\n",
    "        return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f1c20e8-2768-4f5b-bacb-465eaa63181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HDFS client\n",
    "hdfs = HDFSClient(host='localhost', namenode_port=9870, datanode_port=9865)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b648e33-39e6-462d-824a-e0270987a654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV line by line\n",
    "from io import StringIO\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99ca81e2-61ff-4a49-acb7-73b70de0b89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hdfs_streaming(hdfs_path, nrows=None):\n",
    "    content = hdfs.read_file(hdfs_path) \n",
    "    \n",
    "    text_content = content.decode('utf-8')\n",
    "    csv_reader = csv.reader(StringIO(text_content))\n",
    "    \n",
    "    # Get headers\n",
    "    headers = next(csv_reader)\n",
    "    \n",
    "    # Convert rows to Spark Rows\n",
    "    rows = []\n",
    "    for i, row in enumerate(csv_reader):\n",
    "        if nrows and i >= nrows: break\n",
    "        rows.append(Row(**dict(zip(headers, row))))\n",
    "    \n",
    "    # Create DataFrame from Rows\n",
    "    df_spark = spark.createDataFrame(rows)\n",
    "    \n",
    "    return df_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57a1b5d1-f638-4a86-8156-88d98a9d1aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|GRADE|LOSAL|HISAL|\n",
      "+-----+-----+-----+\n",
      "|    1|  700| 1200|\n",
      "|    2| 1201| 1400|\n",
      "|    3| 1401| 2000|\n",
      "|    4| 2001| 3000|\n",
      "|    5| 3001| 9999|\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "df = read_hdfs_streaming('/user/salgrade.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d7a6bc3-8040-420e-82eb-5de869a8dd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Row count: 5\n",
      "Columns: ['GRADE', 'LOSAL', 'HISAL']\n",
      "\n",
      "Schema:\n",
      "root\n",
      " |-- GRADE: string (nullable = true)\n",
      " |-- LOSAL: string (nullable = true)\n",
      " |-- HISAL: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nRow count: {df.count()}\")\n",
    "print(f\"Columns: {df.columns}\")\n",
    "print(\"\\nSchema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5021d43-3c9f-4ddc-951f-39c64a4239f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered data (GRADE > 2):\n",
      "+-----+-----+-----+\n",
      "|GRADE|LOSAL|HISAL|\n",
      "+-----+-----+-----+\n",
      "|    3| 1401| 2000|\n",
      "|    4| 2001| 3000|\n",
      "|    5| 3001| 9999|\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter\n",
    "print(\"\\nFiltered data (GRADE > 2):\")\n",
    "df.filter(df.GRADE > 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82cdb950-19e3-4aea-b11a-eb212c5abe7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SQL Query Result:\n",
      "+-----+-----+-----+\n",
      "|GRADE|LOSAL|HISAL|\n",
      "+-----+-----+-----+\n",
      "|    4| 2001| 3000|\n",
      "|    5| 3001| 9999|\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL Query\n",
    "df.createOrReplaceTempView(\"salgrade\")\n",
    "result = spark.sql(\"SELECT * FROM salgrade WHERE hisal > 2000\")\n",
    "print(\"\\nSQL Query Result:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f606a59-c379-4aed-9a45-1af799a2b7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregation:\n",
      "+-----+-----+\n",
      "|grade|count|\n",
      "+-----+-----+\n",
      "|    1|    1|\n",
      "|    2|    1|\n",
      "|    3|    1|\n",
      "|    4|    1|\n",
      "|    5|    1|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregation\n",
    "print(\"\\nAggregation:\")\n",
    "df.groupBy(\"grade\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8216a1a8-3cd9-4afc-b685-44787dae17c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad90766a-7af2-43ba-84b4-b3e5c0a477dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "1bd1409c-3d14-47c4-9093-ba048576b6e0",
   "metadata": {},
   "source": [
    "# Convert to Pandas if needed\n",
    "df_pandas_result = df.toPandas()\n",
    "print(f\"\\nConverted back to Pandas: {type(df_pandas_result)}\")\n",
    "print(df_pandas_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449d34c9-ffa8-48b3-baa4-13fdcc5bdb7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab4db08-f476-4d98-9283-d7765a2acbe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579183ff-0da4-4e08-bbae-33990884146c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6aea23-d0d8-46e5-a7d7-ed7dbb311cee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235f82a3-e401-43f6-bfbe-dd64f6dae412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-spark310]",
   "language": "python",
   "name": "conda-env-.conda-spark310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
